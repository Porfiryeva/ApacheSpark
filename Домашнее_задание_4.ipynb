{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2hfVwCGAwt9",
    "outputId": "ba760020-9994-465e-aa7f-2b0633bae90a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 47 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 57.4 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=e13bbd8a92ef0a2480f757011f3f958f9535d918d09cca2a8a55d64046569c11\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E3Q9g_UyNxS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/15 16:34:18 WARN Utils: Your hostname, ubuntu-d resolves to a loopback address: 127.0.1.1; using 192.168.0.103 instead (on interface enp7s0)\n",
      "23/01/15 16:34:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/15 16:34:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .appName(\"Lesson_5\")\\\n",
    "    .config(\"spark.executor.instances\",2)\\\n",
    "    .config(\"spark.executor.memory\",'2g')\\\n",
    "    .config(\"spark.executor.cores\",1)\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnXLtjUr_FkX"
   },
   "source": [
    "# Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGioDDCQ_V_-"
   },
   "source": [
    "Оконная функция - функция, которая работает с выделенным набором строк (окном, партицией) и выполняет вычисление для этого набора строк в отдельном столбце. \n",
    "\n",
    "Партиции (окна из набора строк) - это набор строк, указанный для оконной функции по одному из столбцов или группе столбцов таблицы. Партиции для каждой оконной функции в запросе могут быть разделены по различным колонкам таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6tdUDN5_Ee0",
    "outputId": "826d0109-41f4-4122-8d67-d8b1ec62e8aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Предположим, что есть таблицы:\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data =[\n",
    "    (\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"),\\\n",
    "    (\"Orange\",2000,\"USA\"), (\"Orange\",2000,\"USA\"), (\"Banana\",400,\"China\"),\\\n",
    "    (\"Carrots\",1200,\"China\"), (\"Beans\",1500,\"China\"), (\"Orange\",4000,\"China\"),\\\n",
    "    (\"Banana\",2000,\"Canada\"), (\"Carrots\",2000,\"Canada\"), (\"Beans\",2000,\"Mexico\")\n",
    "    ]\n",
    "columns = [\"Product\",\"Amount\", \"Country\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jhq8wrA8BNia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+---+\n",
      "|Product|Amount|Country| rn|\n",
      "+-------+------+-------+---+\n",
      "| Banana|  2000| Canada|  1|\n",
      "|Carrots|  2000| Canada|  2|\n",
      "| Banana|   400|  China|  1|\n",
      "|Carrots|  1200|  China|  2|\n",
      "|  Beans|  1500|  China|  3|\n",
      "| Orange|  4000|  China|  4|\n",
      "|  Beans|  2000| Mexico|  1|\n",
      "| Banana|  1000|    USA|  1|\n",
      "|Carrots|  1500|    USA|  2|\n",
      "|  Beans|  1600|    USA|  3|\n",
      "| Orange|  2000|    USA|  4|\n",
      "| Orange|  2000|    USA|  5|\n",
      "+-------+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# в sql оконная функция записывается следующим образом\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "spark.sql('''\n",
    "select *, \n",
    "row_number() over( partition by Country order by Amount ) as rn from df\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9grWEK6CBbBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+---+\n",
      "|Product|Amount|Country| rn|\n",
      "+-------+------+-------+---+\n",
      "| Banana|  2000| Canada|  1|\n",
      "|Carrots|  2000| Canada|  2|\n",
      "| Banana|   400|  China|  1|\n",
      "|Carrots|  1200|  China|  2|\n",
      "|  Beans|  1500|  China|  3|\n",
      "| Orange|  4000|  China|  4|\n",
      "|  Beans|  2000| Mexico|  1|\n",
      "| Banana|  1000|    USA|  1|\n",
      "|Carrots|  1500|    USA|  2|\n",
      "|  Beans|  1600|    USA|  3|\n",
      "| Orange|  2000|    USA|  4|\n",
      "| Orange|  2000|    USA|  5|\n",
      "+-------+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# в спарке следующим образом\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "windSpec = Window()\\\n",
    "    .partitionBy('Country')\\\n",
    "    .orderBy('Amount')\n",
    "    # .rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n",
    "\n",
    "df.withColumn('rn', F.row_number().over(windSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVGNGR7pN1KC"
   },
   "source": [
    "# Самостоятельная работа к уроку\n",
    "На уроке мы попробовали оконные и пользовательские функции. Теперь закрепим полученные знания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agigNChqOHnK"
   },
   "source": [
    "## Данные: [google drive: raw_sales.csv](https://drive.google.com/file/d/1G2N7Mnt4-Tqz4JdJxutGDMbJiOr32kZp/view?usp=sharing)\n",
    "\n",
    " Каждая строчка это продажа жилья, которая состоит из следующих полей (думаю описание не требуется):\n",
    "*   date of sale\n",
    "*   price\n",
    "*   property type\n",
    "*   number of bedrooms\n",
    "*   4digit postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-36e65zdEEh",
    "outputId": "e0f5e403-08ee-4656-bc13-c5bf65380081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-23 17:45:26--  https://drive.google.com/uc?export=download&id=1xbtFBPz50OBoyYFVGd-B03pCTJdCax07\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.134.100, 74.125.134.101, 74.125.134.139, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.134.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-5k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/985fbke648pmgb8v0kmclemkqv6nsg1k/1663955100000/04099736791713398091/*/1xbtFBPz50OBoyYFVGd-B03pCTJdCax07?e=download&uuid=dda96373-c78e-47fb-8000-c4a2ddcc7543 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-09-23 17:45:27--  https://doc-0c-5k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/985fbke648pmgb8v0kmclemkqv6nsg1k/1663955100000/04099736791713398091/*/1xbtFBPz50OBoyYFVGd-B03pCTJdCax07?e=download&uuid=dda96373-c78e-47fb-8000-c4a2ddcc7543\n",
      "Resolving doc-0c-5k-docs.googleusercontent.com (doc-0c-5k-docs.googleusercontent.com)... 74.125.196.132, 2607:f8b0:400c:c36::84\n",
      "Connecting to doc-0c-5k-docs.googleusercontent.com (doc-0c-5k-docs.googleusercontent.com)|74.125.196.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1505497 (1.4M) [text/csv]\n",
      "Saving to: ‘raw_sales.csv’\n",
      "\n",
      "raw_sales.csv       100%[===================>]   1.44M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2022-09-23 17:45:27 (125 MB/s) - ‘raw_sales.csv’ saved [1505497/1505497]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget 'https://drive.google.com/uc?export=download&id=1xbtFBPz50OBoyYFVGd-B03pCTJdCax07' -O raw_sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpWic7isdH6R",
    "outputId": "d2e62be4-2f78-47e0-b167-3c17811f70fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datesold: timestamp (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- propertyType: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      "\n",
      "+-------------------+--------+-------+------------+--------+\n",
      "|datesold           |postcode|price  |propertyType|bedrooms|\n",
      "+-------------------+--------+-------+------------+--------+\n",
      "|2007-02-07 00:00:00|2607    |525000 |house       |4       |\n",
      "|2007-02-27 00:00:00|2906    |290000 |house       |3       |\n",
      "|2007-03-07 00:00:00|2905    |328000 |house       |3       |\n",
      "|2007-03-09 00:00:00|2905    |380000 |house       |4       |\n",
      "|2007-03-21 00:00:00|2906    |310000 |house       |3       |\n",
      "|2007-04-04 00:00:00|2905    |465000 |house       |4       |\n",
      "|2007-04-24 00:00:00|2607    |399000 |house       |3       |\n",
      "|2007-04-30 00:00:00|2606    |1530000|house       |4       |\n",
      "|2007-05-24 00:00:00|2902    |359000 |house       |3       |\n",
      "|2007-05-25 00:00:00|2906    |320000 |house       |3       |\n",
      "|2007-06-26 00:00:00|2902    |385000 |house       |3       |\n",
      "|2007-06-27 00:00:00|2906    |305000 |house       |3       |\n",
      "|2007-06-27 00:00:00|2612    |850000 |house       |4       |\n",
      "|2007-06-28 00:00:00|2904    |765000 |house       |4       |\n",
      "|2007-06-30 00:00:00|2615    |517000 |house       |4       |\n",
      "|2007-07-02 00:00:00|2914    |800000 |house       |5       |\n",
      "|2007-07-03 00:00:00|2906    |336000 |house       |3       |\n",
      "|2007-07-06 00:00:00|2615    |535000 |house       |5       |\n",
      "|2007-07-07 00:00:00|2602    |900000 |house       |4       |\n",
      "|2007-07-08 00:00:00|2600    |327000 |house       |1       |\n",
      "+-------------------+--------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('raw_sales.csv', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xisyFowtQgx-"
   },
   "source": [
    "## Задание 1\n",
    "Добавьте к таблице следующие поля:\n",
    "*  Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode)\n",
    "*  Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)\n",
    "*  Стоимость последнего проданного дома до текущего\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 до текущего\n",
    "windSpec_before = (Window().partitionBy('postcode')\n",
    "                         .orderBy('datesold')\n",
    "                         .rowsBetween(Window.currentRow - 10, Window.currentRow - 1)\n",
    "                  )\n",
    "\n",
    "# 10 после текущего\n",
    "windSpec_after = (Window().partitionBy('postcode')\n",
    "                          .orderBy('datesold')\n",
    "                          .rowsBetween(Window.currentRow + 1, Window.currentRow + 10)\n",
    "                 )\n",
    "\n",
    "# последний до текущего\n",
    "windSpec_last = (Window().partitionBy('postcode')\n",
    "                         .orderBy('datesold')\n",
    "                )\n",
    "\n",
    "df = df.select('*', \n",
    "               F.avg('price').over(windSpec_before).alias('before'),\n",
    "               F.avg('price').over(windSpec_after).alias('after'),\n",
    "               F.lag('price', 1).over(windSpec_last).alias('last')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+-----------------+--------+-------+\n",
      "|           datesold|postcode|  price|           before|   after|   last|\n",
      "+-------------------+--------+-------+-----------------+--------+-------+\n",
      "|2007-07-08 00:00:00|    2600| 327000|             null|708350.0|   null|\n",
      "|2007-08-16 00:00:00|    2600| 790000|         327000.0|698350.0| 327000|\n",
      "|2007-12-05 00:00:00|    2600| 825000|         558500.0|679350.0| 790000|\n",
      "|2008-01-21 00:00:00|    2600| 315000|647333.3333333334|742850.0| 825000|\n",
      "|2008-04-24 00:00:00|    2600| 292500|         564250.0|786600.0| 315000|\n",
      "|2008-05-30 00:00:00|    2600| 329000|         509900.0|839200.0| 292500|\n",
      "|2008-06-19 00:00:00|    2600| 765000|         479750.0|868450.0| 329000|\n",
      "|2008-07-29 00:00:00|    2600| 927000|         520500.0|805750.0| 765000|\n",
      "|2008-09-02 00:00:00|    2600|1380000|         571312.5|715250.0| 927000|\n",
      "|2008-09-08 00:00:00|    2600| 740000|661166.6666666666|756250.0|1380000|\n",
      "|2008-09-17 00:00:00|    2600| 720000|         669050.0|741750.0| 740000|\n",
      "|2008-09-22 00:00:00|    2600| 690000|         708350.0|730550.0| 720000|\n",
      "|2008-11-18 00:00:00|    2600| 635000|         698350.0|755050.0| 690000|\n",
      "|2008-11-18 00:00:00|    2600| 950000|         679350.0|701050.0| 635000|\n",
      "|2008-11-21 00:00:00|    2600| 730000|         742850.0|729550.0| 950000|\n",
      "|2008-12-22 00:00:00|    2600| 855000|         786600.0|716250.0| 730000|\n",
      "|2008-12-24 00:00:00|    2600|1057500|         839200.0|641500.0| 855000|\n",
      "|2009-01-06 00:00:00|    2600| 300000|         868450.0|672500.0|1057500|\n",
      "|2009-01-12 00:00:00|    2600| 475000|         805750.0|689000.0| 300000|\n",
      "|2009-01-20 00:00:00|    2600|1150000|         715250.0|641500.0| 475000|\n",
      "+-------------------+--------+-------+-----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('datesold', 'postcode', 'price', 'before', 'after', 'last').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvh2x6_8YU3F"
   },
   "source": [
    "## Задание 2\n",
    "В итоге у вас таблица с колонками (или нечто похожее):\n",
    "*   price\n",
    "*   Среднегодовая цена\n",
    "*  Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode) (1 балл)\n",
    "*  Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode) (1 балл)\n",
    "*  Стоимость последнего проданного дома до текущего ((1 балл)\n",
    "*  и др.\n",
    "\n",
    "Посчитайте кол-во уникальных значений в каждой строчке (unique(row)) (ипользуйте udf). Попробуйте сделать то же самое используя pandas udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinkt_val|\n",
      "+------------+\n",
      "|           7|\n",
      "|           8|\n",
      "|           8|\n",
      "|           8|\n",
      "|           8|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# udf\n",
    "@F.udf(returnType=IntegerType())\n",
    "def row_distinct(row):\n",
    "    return len(set(row))\n",
    "\n",
    "\n",
    "df.select(row_distinct(F.array('*')).alias('distinkt_val')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_val|\n",
      "+------------+\n",
      "|           7|\n",
      "|           7|\n",
      "|           8|\n",
      "|           8|\n",
      "|           8|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@F.pandas_udf(IntegerType())\n",
    "def pd_row_distinct(*row):\n",
    "    tmp = pd.concat([*row], axis=1).apply(lambda x: x.unique().size, axis=1)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "df.select(pd_row_distinct(*df.columns).alias('distinct_val')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSZTI9PAwQb"
   },
   "source": [
    "# Задание 3\n",
    "SQL like case when или if elif else\n",
    "\n",
    "Создайте колонку, в которой в которой будет отображаться \"+\", \"-\" или \"=\", если \"Средняя стомость 10 проданных домов до текущего в том же районе\" больше, меньше или равно \"Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)\", соотвественно.\n",
    "\n",
    "Если одно из полей Null, запишите в эту колонку \"Нет данных\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rfOBthog-2Q",
    "outputId": "b40b0a47-7677-41a0-a779-70ccc8568c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------+-------+----------+\n",
      "|  price|           before|   after|   last|        if|\n",
      "+-------+-----------------+--------+-------+----------+\n",
      "| 327000|             null|708350.0|   null|Нет данных|\n",
      "| 790000|         327000.0|698350.0| 327000|         -|\n",
      "| 825000|         558500.0|679350.0| 790000|         -|\n",
      "| 315000|647333.3333333334|742850.0| 825000|         -|\n",
      "| 292500|         564250.0|786600.0| 315000|         -|\n",
      "| 329000|         509900.0|839200.0| 292500|         -|\n",
      "| 765000|         479750.0|868450.0| 329000|         -|\n",
      "| 927000|         520500.0|805750.0| 765000|         -|\n",
      "|1380000|         571312.5|715250.0| 927000|         -|\n",
      "| 740000|661166.6666666666|756250.0|1380000|         -|\n",
      "| 720000|         669050.0|741750.0| 740000|         -|\n",
      "| 690000|         708350.0|730550.0| 720000|         -|\n",
      "| 635000|         698350.0|755050.0| 690000|         -|\n",
      "| 950000|         679350.0|701050.0| 635000|         -|\n",
      "| 730000|         742850.0|729550.0| 950000|         +|\n",
      "| 855000|         786600.0|716250.0| 730000|         +|\n",
      "|1057500|         839200.0|641500.0| 855000|         +|\n",
      "| 300000|         868450.0|672500.0|1057500|         +|\n",
      "| 475000|         805750.0|689000.0| 300000|         +|\n",
      "|1150000|         715250.0|641500.0| 475000|         +|\n",
      "+-------+-----------------+--------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.select('price', 'before', 'after', 'last')\n",
    "   .withColumn('if', \n",
    "               F.when(df.before > df.after, F.lit('+'))\n",
    "                .when(df.before < df.after, F.lit('-'))\n",
    "                .when(df.before == df.after, F.lit('='))\n",
    "                .otherwise(F.lit('Нет данных')))\n",
    "   .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2q7HSEqWBVn"
   },
   "source": [
    "# Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3pfUThFQtE6",
    "outputId": "953b20ec-4cc2-4ea3-b133-c34f4c06120f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "+-----+-------+-------+\n",
      "|key_1|value_1|value_2|\n",
      "+-----+-------+-------+\n",
      "|  abc|     10|     20|\n",
      "|  def|    100|    300|\n",
      "+-----+-------+-------+\n",
      "\n",
      "df2\n",
      "+-----+-------+-------+\n",
      "|key_2|value_1|value_2|\n",
      "+-----+-------+-------+\n",
      "|  abc|    5.5|    2.2|\n",
      "|  xyz|   10.1|   13.5|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создаём датасет для примеров\n",
    "dataset_1 = [\n",
    "  {\n",
    "    'key_1' : 'abc',\n",
    "    'value_1' : 10,\n",
    "    'value_2' : 20\n",
    "  },\n",
    "  {\n",
    "    'key_1' : 'def',\n",
    "    'value_1' : 100,\n",
    "    'value_2' : 300\n",
    "  }\n",
    "]\n",
    "\n",
    "dataset_2 = [\n",
    "  {\n",
    "    'key_2' : 'abc',\n",
    "    'value_1' : 5.5,\n",
    "    'value_2' : 2.2\n",
    "  },\n",
    "  {\n",
    "    'key_2' : 'xyz',\n",
    "    'value_1' : 10.1,\n",
    "    'value_2' : 13.5\n",
    "  }\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(dataset_1)\n",
    "print('df1')\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(dataset_2)\n",
    "print('df2')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0E_1iKeWFeY"
   },
   "source": [
    "## Задание 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od3V0xa4V7U8"
   },
   "outputs": [],
   "source": [
    "# Создайте джойн, чтобы получить следующую таблицу\n",
    "# +---+-------+-------+\n",
    "# |key|value_1|value_2|\n",
    "# +---+-------+-------+\n",
    "# |abc|     10|     20|\n",
    "# +---+-------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRDcZgNKjZru",
    "outputId": "01608368-73f3-4a5a-e273-7bfdfc4139e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|key_1|value_1|value_2|\n",
      "+-----+-------+-------+\n",
      "|  abc|     10|     20|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=df1.key_1 == df2.key_2, how='leftsemi').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrwYa2ZAWYmF"
   },
   "source": [
    "## Задание 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEb-gGG8WQST"
   },
   "outputs": [],
   "source": [
    "# Создайте джойн, чтобы получить следующую таблицу\n",
    "# +---+-------+-------+\n",
    "# |key|value_1|value_2|\n",
    "# +---+-------+-------+\n",
    "# |def|    100|    300|\n",
    "# +---+-------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKM1D_FIjBc9",
    "outputId": "be771f49-2ae9-43a3-d7fd-dd7e07e8370f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|key_1|value_1|value_2|\n",
      "+-----+-------+-------+\n",
      "|  def|    100|    300|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=df1.key_1 == df2.key_2, how='leftanti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbzsGu30WZTb"
   },
   "source": [
    "## Задание 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ84QTURWWY_"
   },
   "outputs": [],
   "source": [
    "# Создайте Inner джойн с условиями ---hidden---, для df1 и df2, соответсвенно\n",
    "# В  итоге получится таблица\n",
    "# +---+-------+-------+---+-------+-------+\n",
    "# |key|value_1|value_2|key|value_1|value_2|\n",
    "# +---+-------+-------+---+-------+-------+\n",
    "# |abc|     10|     20|abc|    5.5|    2.2|\n",
    "# |abc|     10|     20|xyz|   10.1|   13.5|\n",
    "# |def|    100|    300|abc|    5.5|    2.2|\n",
    "# |def|    100|    300|xyz|   10.1|   13.5|\n",
    "# +---+-------+-------+---+-------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zzGUJN7inrF",
    "outputId": "3e53425a-4e8d-43a6-eb49-3cef094a687b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+-----+-------+-------+\n",
      "|key_1|value_1|value_2|key_2|value_1|value_2|\n",
      "+-----+-------+-------+-----+-------+-------+\n",
      "|  abc|     10|     20|  abc|    5.5|    2.2|\n",
      "|  abc|     10|     20|  xyz|   10.1|   13.5|\n",
      "|  def|    100|    300|  abc|    5.5|    2.2|\n",
      "|  def|    100|    300|  xyz|   10.1|   13.5|\n",
      "+-----+-------+-------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=df1.value_2 > df2.value_2, how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
