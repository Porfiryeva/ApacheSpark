{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6W7u-73y5f5g"
   },
   "source": [
    "Здравствуйте.\n",
    "\n",
    "# Задание\n",
    "Ниже обучается и оцениваться модель. \n",
    "\n",
    "Нужно перевести этот в Pipeline (вам понадобится VectorAssembler), а затем оценить MAE с помощью spark. И попробовать потюнить параметры модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "wH9YYYd9yHM6"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "sW4oDIDPw5Dd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes, load_iris, load_boston\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .appName(\"Lesson_2\")\\\n",
    "    .config(\"spark.executor.instances\",2)\\\n",
    "    .config(\"spark.executor.memory\",'2g')\\\n",
    "    .config(\"spark.executor.cores\",1)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "9wpYw5zUxBzv"
   },
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "dataset = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "dataset['target'] = data['target']\n",
    "\n",
    "cols_to_vector = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "spark_dataset = spark.createDataFrame(dataset).select(cols_to_vector(F.array(*data['feature_names'])).alias('features'), 'target').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "HYdMGKWaxqsI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, test = spark_dataset.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = RandomForestRegressor(labelCol='target')\n",
    "lr = lr.fit(train)\n",
    "train_predictions = lr.transform(train)\n",
    "test_predictions = lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUBg1pAR3dpT",
    "outputId": "0d91a044-9a66-4256-bd02-887706d9cd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Scores:: \n",
      "        train: 1.9248439814434293, \n",
      "        test: 2.1745401466446843\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Заменить нужно эту часть\n",
    "pandas_train_predictions = train_predictions.toPandas()\n",
    "pandas_test_predictions = test_predictions.toPandas()\n",
    "\n",
    "print(f'''\n",
    "    Scores:: \n",
    "        train: {mean_absolute_error(\n",
    "            pandas_train_predictions.target, \n",
    "            pandas_train_predictions.prediction)}, \n",
    "        test: {mean_absolute_error(\n",
    "            pandas_test_predictions.target, \n",
    "            pandas_test_predictions.prediction)}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RaQVvGCM72e"
   },
   "source": [
    "Используйте RegressionEvaluator. Evaluation ипортируется следующим образом:\n",
    "\n",
    "\n",
    "```\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "```\n",
    "\n",
    "В частности [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html#pyspark.ml.evaluation.RegressionEvaluator.metricName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXiVZ_qkNLlt"
   },
   "source": [
    "# 1 Переведите в пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = RandomForestRegressor(labelCol='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "D2j2Iuo8NCit"
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[lr])\n",
    "fitted_pipe = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|            features|target|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|[0.01311,90.0,1.2...|  35.4| 33.59250373927148|\n",
      "|[0.0136,75.0,4.0,...|  18.9|20.605932795213498|\n",
      "|[0.01381,80.0,0.4...|  50.0| 46.90220697080697|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+------+------------------+\n",
      "|            features|target|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|[0.00632,18.0,2.3...|  24.0|28.985415548361846|\n",
      "|[0.01432,100.0,1....|  31.6| 31.05084902873346|\n",
      "|[0.01778,95.0,1.4...|  32.9|  33.1172993200348|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = fitted_pipe.transform(train)\n",
    "test_pred = fitted_pipe.transform(test)\n",
    "\n",
    "train_pred.show(3), test_pred.show(3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlb35esQNWcf"
   },
   "source": [
    "# 2 Оцените модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mae: 1.92\n",
      "test mae: 2.17\n"
     ]
    }
   ],
   "source": [
    "regr_eval = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='target',\n",
    "                                metricName='mae'\n",
    "                               )\n",
    "\n",
    "print(f'train mae: {regr_eval.evaluate(train_pred):0.2f}\\ntest mae: {regr_eval.evaluate(test_pred):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G2kKw9WOLMt"
   },
   "source": [
    "# 3 Улучшите модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNyxk11NOYyz"
   },
   "source": [
    "В этом вам может помочь gridsearch\n",
    "\n",
    "```\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "\n",
    "paramgrid = ParamGridBuilder()\\. \n",
    ".addGrid()\\. \n",
    ".addGrid().build(). \n",
    "TrainValidationSplit(\n",
    "    estimator = pipe,  \n",
    "    evaluator = evaluator,   \n",
    "    estimatorParamMaps = paramgrid). \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "sjgXUYMSOM40"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid = (ParamGridBuilder().addGrid(lr.maxDepth, [5, 7, 12])\n",
    "                               .addGrid(lr.numTrees, [15, 20, 25])\n",
    "                               .addGrid(lr.subsamplingRate, [0.6, 0.8, 1])\n",
    "                               .build()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=lr, \n",
    "                           estimatorParamMaps=paramgrid, \n",
    "                           evaluator=regr_eval,\n",
    "                           parallelism=2, \n",
    "                           seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/22 14:46:37 WARN DAGScheduler: Broadcasting large task binary with size 1065.6 KiB\n"
     ]
    }
   ],
   "source": [
    "tvsModel = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3543210729703876,\n",
       " 2.284143063004749,\n",
       " 2.184314968620492,\n",
       " 2.278105813138483,\n",
       " 2.283447069275894,\n",
       " 2.2486400987772868,\n",
       " 2.2527475972267603,\n",
       " 2.289586170874206,\n",
       " 2.367354168127326,\n",
       " 2.3540486839871444,\n",
       " 2.238095361496401,\n",
       " 2.1644573980245063,\n",
       " 2.218304560941528,\n",
       " 2.403887143839786,\n",
       " 2.145432628748238,\n",
       " 2.1546647509663437,\n",
       " 2.256084084330268,\n",
       " 2.3299761946014352,\n",
       " 2.3665339159384113,\n",
       " 2.1516069796400266,\n",
       " 2.18613849030703,\n",
       " 2.1620009971302103,\n",
       " 2.3195837713111516,\n",
       " 2.1220314837499648,\n",
       " 2.0878127246037352,\n",
       " 2.251026249293778,\n",
       " 2.3056384295853958]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvsModel.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_i = tvsModel.validationMetrics.index(min(tvsModel.validationMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestRegressor_01703731615d', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12,\n",
       " Param(parent='RandomForestRegressor_01703731615d', name='numTrees', doc='Number of trees to train (>= 1).'): 25,\n",
       " Param(parent='RandomForestRegressor_01703731615d', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 0.6}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramgrid[best_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'maxDepth': 12, 'numTrees': 25, 'subsamplingRate': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print('best params: ', dict(zip(['maxDepth', 'numTrees', 'subsamplingRate'], [paramgrid[best_i][key] for key in paramgrid[best_i]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трейн улучшился, тест - немного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3983229983020642, 2.0786862820981837)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_eval.evaluate(tvsModel.transform(train)), regr_eval.evaluate(tvsModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
